# Flash Attention X

Flash Attention X is a flash attention(s) implementation using Triton, including flash_attention_full, flash_attention_causal and flash_attention_bias...

## Installation

You can install Flash Attention X using pip:
```bash
pip install flash_attention_x
```

## Requirements

- Python >= 3.8
- Triton >= 2.3.0

The package is tested with Triton 2.3.0 and CUDA 12.0.

## Features

- Efficient implementation of flash attention(s), including flash_attention_full, flash_attention_causal and flash_attention_bias...
- Built using Triton for optimized performance

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Author

Xiaotian Han
